// Code generated by mdatagen. DO NOT EDIT.

package metadata

import (
	"time"

	"go.opentelemetry.io/collector/model/pdata"
)

// MetricSettings provides common settings for a particular metric.
type MetricSettings struct {
	Enabled bool `mapstructure:"enabled"`
}

// MetricsSettings provides settings for kafkametricsreceiver metrics.
type MetricsSettings struct {
	KafkaBrokers                 MetricSettings `mapstructure:"kafka.brokers"`
	KafkaConsumerGroupLag        MetricSettings `mapstructure:"kafka.consumer_group.lag"`
	KafkaConsumerGroupLagSum     MetricSettings `mapstructure:"kafka.consumer_group.lag_sum"`
	KafkaConsumerGroupMembers    MetricSettings `mapstructure:"kafka.consumer_group.members"`
	KafkaConsumerGroupOffset     MetricSettings `mapstructure:"kafka.consumer_group.offset"`
	KafkaConsumerGroupOffsetSum  MetricSettings `mapstructure:"kafka.consumer_group.offset_sum"`
	KafkaPartitionCurrentOffset  MetricSettings `mapstructure:"kafka.partition.current_offset"`
	KafkaPartitionOldestOffset   MetricSettings `mapstructure:"kafka.partition.oldest_offset"`
	KafkaPartitionReplicas       MetricSettings `mapstructure:"kafka.partition.replicas"`
	KafkaPartitionReplicasInSync MetricSettings `mapstructure:"kafka.partition.replicas_in_sync"`
	KafkaTopicPartitions         MetricSettings `mapstructure:"kafka.topic.partitions"`
}

func DefaultMetricsSettings() MetricsSettings {
	return MetricsSettings{
		KafkaBrokers: MetricSettings{
			Enabled: true,
		},
		KafkaConsumerGroupLag: MetricSettings{
			Enabled: true,
		},
		KafkaConsumerGroupLagSum: MetricSettings{
			Enabled: true,
		},
		KafkaConsumerGroupMembers: MetricSettings{
			Enabled: true,
		},
		KafkaConsumerGroupOffset: MetricSettings{
			Enabled: true,
		},
		KafkaConsumerGroupOffsetSum: MetricSettings{
			Enabled: true,
		},
		KafkaPartitionCurrentOffset: MetricSettings{
			Enabled: true,
		},
		KafkaPartitionOldestOffset: MetricSettings{
			Enabled: true,
		},
		KafkaPartitionReplicas: MetricSettings{
			Enabled: true,
		},
		KafkaPartitionReplicasInSync: MetricSettings{
			Enabled: true,
		},
		KafkaTopicPartitions: MetricSettings{
			Enabled: true,
		},
	}
}

type metricKafkaBrokers struct {
	data     pdata.Metric   // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills kafka.brokers metric with initial data.
func (m *metricKafkaBrokers) init() {
	m.data.SetName("kafka.brokers")
	m.data.SetDescription("Number of brokers in the cluster.")
	m.data.SetUnit("{brokers}")
	m.data.SetDataType(pdata.MetricDataTypeGauge)
}

func (m *metricKafkaBrokers) recordDataPoint(start pdata.Timestamp, ts pdata.Timestamp, val int64) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntVal(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricKafkaBrokers) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricKafkaBrokers) emit(metrics pdata.MetricSlice) {
	if m.settings.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricKafkaBrokers(settings MetricSettings) metricKafkaBrokers {
	m := metricKafkaBrokers{settings: settings}
	if settings.Enabled {
		m.data = pdata.NewMetric()
		m.init()
	}
	return m
}

type metricKafkaConsumerGroupLag struct {
	data     pdata.Metric   // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills kafka.consumer_group.lag metric with initial data.
func (m *metricKafkaConsumerGroupLag) init() {
	m.data.SetName("kafka.consumer_group.lag")
	m.data.SetDescription("Current approximate lag of consumer group at partition of topic")
	m.data.SetUnit("1")
	m.data.SetDataType(pdata.MetricDataTypeGauge)
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricKafkaConsumerGroupLag) recordDataPoint(start pdata.Timestamp, ts pdata.Timestamp, val int64, groupAttributeValue string, topicAttributeValue string, partitionAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntVal(val)
	dp.Attributes().Insert(A.Group, pdata.NewAttributeValueString(groupAttributeValue))
	dp.Attributes().Insert(A.Topic, pdata.NewAttributeValueString(topicAttributeValue))
	dp.Attributes().Insert(A.Partition, pdata.NewAttributeValueString(partitionAttributeValue))
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricKafkaConsumerGroupLag) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricKafkaConsumerGroupLag) emit(metrics pdata.MetricSlice) {
	if m.settings.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricKafkaConsumerGroupLag(settings MetricSettings) metricKafkaConsumerGroupLag {
	m := metricKafkaConsumerGroupLag{settings: settings}
	if settings.Enabled {
		m.data = pdata.NewMetric()
		m.init()
	}
	return m
}

type metricKafkaConsumerGroupLagSum struct {
	data     pdata.Metric   // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills kafka.consumer_group.lag_sum metric with initial data.
func (m *metricKafkaConsumerGroupLagSum) init() {
	m.data.SetName("kafka.consumer_group.lag_sum")
	m.data.SetDescription("Current approximate sum of consumer group lag across all partitions of topic")
	m.data.SetUnit("1")
	m.data.SetDataType(pdata.MetricDataTypeGauge)
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricKafkaConsumerGroupLagSum) recordDataPoint(start pdata.Timestamp, ts pdata.Timestamp, val int64, groupAttributeValue string, topicAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntVal(val)
	dp.Attributes().Insert(A.Group, pdata.NewAttributeValueString(groupAttributeValue))
	dp.Attributes().Insert(A.Topic, pdata.NewAttributeValueString(topicAttributeValue))
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricKafkaConsumerGroupLagSum) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricKafkaConsumerGroupLagSum) emit(metrics pdata.MetricSlice) {
	if m.settings.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricKafkaConsumerGroupLagSum(settings MetricSettings) metricKafkaConsumerGroupLagSum {
	m := metricKafkaConsumerGroupLagSum{settings: settings}
	if settings.Enabled {
		m.data = pdata.NewMetric()
		m.init()
	}
	return m
}

type metricKafkaConsumerGroupMembers struct {
	data     pdata.Metric   // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills kafka.consumer_group.members metric with initial data.
func (m *metricKafkaConsumerGroupMembers) init() {
	m.data.SetName("kafka.consumer_group.members")
	m.data.SetDescription("Count of members in the consumer group")
	m.data.SetUnit("{members}")
	m.data.SetDataType(pdata.MetricDataTypeGauge)
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricKafkaConsumerGroupMembers) recordDataPoint(start pdata.Timestamp, ts pdata.Timestamp, val int64, groupAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntVal(val)
	dp.Attributes().Insert(A.Group, pdata.NewAttributeValueString(groupAttributeValue))
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricKafkaConsumerGroupMembers) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricKafkaConsumerGroupMembers) emit(metrics pdata.MetricSlice) {
	if m.settings.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricKafkaConsumerGroupMembers(settings MetricSettings) metricKafkaConsumerGroupMembers {
	m := metricKafkaConsumerGroupMembers{settings: settings}
	if settings.Enabled {
		m.data = pdata.NewMetric()
		m.init()
	}
	return m
}

type metricKafkaConsumerGroupOffset struct {
	data     pdata.Metric   // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills kafka.consumer_group.offset metric with initial data.
func (m *metricKafkaConsumerGroupOffset) init() {
	m.data.SetName("kafka.consumer_group.offset")
	m.data.SetDescription("Current offset of the consumer group at partition of topic")
	m.data.SetUnit("1")
	m.data.SetDataType(pdata.MetricDataTypeGauge)
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricKafkaConsumerGroupOffset) recordDataPoint(start pdata.Timestamp, ts pdata.Timestamp, val int64, groupAttributeValue string, topicAttributeValue string, partitionAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntVal(val)
	dp.Attributes().Insert(A.Group, pdata.NewAttributeValueString(groupAttributeValue))
	dp.Attributes().Insert(A.Topic, pdata.NewAttributeValueString(topicAttributeValue))
	dp.Attributes().Insert(A.Partition, pdata.NewAttributeValueString(partitionAttributeValue))
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricKafkaConsumerGroupOffset) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricKafkaConsumerGroupOffset) emit(metrics pdata.MetricSlice) {
	if m.settings.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricKafkaConsumerGroupOffset(settings MetricSettings) metricKafkaConsumerGroupOffset {
	m := metricKafkaConsumerGroupOffset{settings: settings}
	if settings.Enabled {
		m.data = pdata.NewMetric()
		m.init()
	}
	return m
}

type metricKafkaConsumerGroupOffsetSum struct {
	data     pdata.Metric   // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills kafka.consumer_group.offset_sum metric with initial data.
func (m *metricKafkaConsumerGroupOffsetSum) init() {
	m.data.SetName("kafka.consumer_group.offset_sum")
	m.data.SetDescription("Sum of consumer group offset across partitions of topic")
	m.data.SetUnit("1")
	m.data.SetDataType(pdata.MetricDataTypeGauge)
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricKafkaConsumerGroupOffsetSum) recordDataPoint(start pdata.Timestamp, ts pdata.Timestamp, val int64, groupAttributeValue string, topicAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntVal(val)
	dp.Attributes().Insert(A.Group, pdata.NewAttributeValueString(groupAttributeValue))
	dp.Attributes().Insert(A.Topic, pdata.NewAttributeValueString(topicAttributeValue))
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricKafkaConsumerGroupOffsetSum) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricKafkaConsumerGroupOffsetSum) emit(metrics pdata.MetricSlice) {
	if m.settings.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricKafkaConsumerGroupOffsetSum(settings MetricSettings) metricKafkaConsumerGroupOffsetSum {
	m := metricKafkaConsumerGroupOffsetSum{settings: settings}
	if settings.Enabled {
		m.data = pdata.NewMetric()
		m.init()
	}
	return m
}

type metricKafkaPartitionCurrentOffset struct {
	data     pdata.Metric   // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills kafka.partition.current_offset metric with initial data.
func (m *metricKafkaPartitionCurrentOffset) init() {
	m.data.SetName("kafka.partition.current_offset")
	m.data.SetDescription("Current offset of partition of topic.")
	m.data.SetUnit("1")
	m.data.SetDataType(pdata.MetricDataTypeGauge)
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricKafkaPartitionCurrentOffset) recordDataPoint(start pdata.Timestamp, ts pdata.Timestamp, val int64, topicAttributeValue string, partitionAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntVal(val)
	dp.Attributes().Insert(A.Topic, pdata.NewAttributeValueString(topicAttributeValue))
	dp.Attributes().Insert(A.Partition, pdata.NewAttributeValueString(partitionAttributeValue))
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricKafkaPartitionCurrentOffset) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricKafkaPartitionCurrentOffset) emit(metrics pdata.MetricSlice) {
	if m.settings.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricKafkaPartitionCurrentOffset(settings MetricSettings) metricKafkaPartitionCurrentOffset {
	m := metricKafkaPartitionCurrentOffset{settings: settings}
	if settings.Enabled {
		m.data = pdata.NewMetric()
		m.init()
	}
	return m
}

type metricKafkaPartitionOldestOffset struct {
	data     pdata.Metric   // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills kafka.partition.oldest_offset metric with initial data.
func (m *metricKafkaPartitionOldestOffset) init() {
	m.data.SetName("kafka.partition.oldest_offset")
	m.data.SetDescription("Oldest offset of partition of topic")
	m.data.SetUnit("1")
	m.data.SetDataType(pdata.MetricDataTypeGauge)
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricKafkaPartitionOldestOffset) recordDataPoint(start pdata.Timestamp, ts pdata.Timestamp, val int64, topicAttributeValue string, partitionAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntVal(val)
	dp.Attributes().Insert(A.Topic, pdata.NewAttributeValueString(topicAttributeValue))
	dp.Attributes().Insert(A.Partition, pdata.NewAttributeValueString(partitionAttributeValue))
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricKafkaPartitionOldestOffset) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricKafkaPartitionOldestOffset) emit(metrics pdata.MetricSlice) {
	if m.settings.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricKafkaPartitionOldestOffset(settings MetricSettings) metricKafkaPartitionOldestOffset {
	m := metricKafkaPartitionOldestOffset{settings: settings}
	if settings.Enabled {
		m.data = pdata.NewMetric()
		m.init()
	}
	return m
}

type metricKafkaPartitionReplicas struct {
	data     pdata.Metric   // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills kafka.partition.replicas metric with initial data.
func (m *metricKafkaPartitionReplicas) init() {
	m.data.SetName("kafka.partition.replicas")
	m.data.SetDescription("Number of replicas for partition of topic")
	m.data.SetUnit("{replicas}")
	m.data.SetDataType(pdata.MetricDataTypeGauge)
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricKafkaPartitionReplicas) recordDataPoint(start pdata.Timestamp, ts pdata.Timestamp, val int64, topicAttributeValue string, partitionAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntVal(val)
	dp.Attributes().Insert(A.Topic, pdata.NewAttributeValueString(topicAttributeValue))
	dp.Attributes().Insert(A.Partition, pdata.NewAttributeValueString(partitionAttributeValue))
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricKafkaPartitionReplicas) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricKafkaPartitionReplicas) emit(metrics pdata.MetricSlice) {
	if m.settings.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricKafkaPartitionReplicas(settings MetricSettings) metricKafkaPartitionReplicas {
	m := metricKafkaPartitionReplicas{settings: settings}
	if settings.Enabled {
		m.data = pdata.NewMetric()
		m.init()
	}
	return m
}

type metricKafkaPartitionReplicasInSync struct {
	data     pdata.Metric   // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills kafka.partition.replicas_in_sync metric with initial data.
func (m *metricKafkaPartitionReplicasInSync) init() {
	m.data.SetName("kafka.partition.replicas_in_sync")
	m.data.SetDescription("Number of synchronized replicas of partition")
	m.data.SetUnit("{replicas}")
	m.data.SetDataType(pdata.MetricDataTypeGauge)
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricKafkaPartitionReplicasInSync) recordDataPoint(start pdata.Timestamp, ts pdata.Timestamp, val int64, topicAttributeValue string, partitionAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntVal(val)
	dp.Attributes().Insert(A.Topic, pdata.NewAttributeValueString(topicAttributeValue))
	dp.Attributes().Insert(A.Partition, pdata.NewAttributeValueString(partitionAttributeValue))
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricKafkaPartitionReplicasInSync) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricKafkaPartitionReplicasInSync) emit(metrics pdata.MetricSlice) {
	if m.settings.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricKafkaPartitionReplicasInSync(settings MetricSettings) metricKafkaPartitionReplicasInSync {
	m := metricKafkaPartitionReplicasInSync{settings: settings}
	if settings.Enabled {
		m.data = pdata.NewMetric()
		m.init()
	}
	return m
}

type metricKafkaTopicPartitions struct {
	data     pdata.Metric   // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills kafka.topic.partitions metric with initial data.
func (m *metricKafkaTopicPartitions) init() {
	m.data.SetName("kafka.topic.partitions")
	m.data.SetDescription("Number of partitions in topic.")
	m.data.SetUnit("{partitions}")
	m.data.SetDataType(pdata.MetricDataTypeGauge)
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricKafkaTopicPartitions) recordDataPoint(start pdata.Timestamp, ts pdata.Timestamp, val int64, topicAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntVal(val)
	dp.Attributes().Insert(A.Topic, pdata.NewAttributeValueString(topicAttributeValue))
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricKafkaTopicPartitions) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricKafkaTopicPartitions) emit(metrics pdata.MetricSlice) {
	if m.settings.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricKafkaTopicPartitions(settings MetricSettings) metricKafkaTopicPartitions {
	m := metricKafkaTopicPartitions{settings: settings}
	if settings.Enabled {
		m.data = pdata.NewMetric()
		m.init()
	}
	return m
}

// MetricsBuilder provides an interface for scrapers to report metrics while taking care of all the transformations
// required to produce metric representation defined in metadata and user settings.
type MetricsBuilder struct {
	startTime                          pdata.Timestamp
	metricKafkaBrokers                 metricKafkaBrokers
	metricKafkaConsumerGroupLag        metricKafkaConsumerGroupLag
	metricKafkaConsumerGroupLagSum     metricKafkaConsumerGroupLagSum
	metricKafkaConsumerGroupMembers    metricKafkaConsumerGroupMembers
	metricKafkaConsumerGroupOffset     metricKafkaConsumerGroupOffset
	metricKafkaConsumerGroupOffsetSum  metricKafkaConsumerGroupOffsetSum
	metricKafkaPartitionCurrentOffset  metricKafkaPartitionCurrentOffset
	metricKafkaPartitionOldestOffset   metricKafkaPartitionOldestOffset
	metricKafkaPartitionReplicas       metricKafkaPartitionReplicas
	metricKafkaPartitionReplicasInSync metricKafkaPartitionReplicasInSync
	metricKafkaTopicPartitions         metricKafkaTopicPartitions
}

// metricBuilderOption applies changes to default metrics builder.
type metricBuilderOption func(*MetricsBuilder)

// WithStartTime sets startTime on the metrics builder.
func WithStartTime(startTime pdata.Timestamp) metricBuilderOption {
	return func(mb *MetricsBuilder) {
		mb.startTime = startTime
	}
}

func NewMetricsBuilder(settings MetricsSettings, options ...metricBuilderOption) *MetricsBuilder {
	mb := &MetricsBuilder{
		startTime:                          pdata.NewTimestampFromTime(time.Now()),
		metricKafkaBrokers:                 newMetricKafkaBrokers(settings.KafkaBrokers),
		metricKafkaConsumerGroupLag:        newMetricKafkaConsumerGroupLag(settings.KafkaConsumerGroupLag),
		metricKafkaConsumerGroupLagSum:     newMetricKafkaConsumerGroupLagSum(settings.KafkaConsumerGroupLagSum),
		metricKafkaConsumerGroupMembers:    newMetricKafkaConsumerGroupMembers(settings.KafkaConsumerGroupMembers),
		metricKafkaConsumerGroupOffset:     newMetricKafkaConsumerGroupOffset(settings.KafkaConsumerGroupOffset),
		metricKafkaConsumerGroupOffsetSum:  newMetricKafkaConsumerGroupOffsetSum(settings.KafkaConsumerGroupOffsetSum),
		metricKafkaPartitionCurrentOffset:  newMetricKafkaPartitionCurrentOffset(settings.KafkaPartitionCurrentOffset),
		metricKafkaPartitionOldestOffset:   newMetricKafkaPartitionOldestOffset(settings.KafkaPartitionOldestOffset),
		metricKafkaPartitionReplicas:       newMetricKafkaPartitionReplicas(settings.KafkaPartitionReplicas),
		metricKafkaPartitionReplicasInSync: newMetricKafkaPartitionReplicasInSync(settings.KafkaPartitionReplicasInSync),
		metricKafkaTopicPartitions:         newMetricKafkaTopicPartitions(settings.KafkaTopicPartitions),
	}
	for _, op := range options {
		op(mb)
	}
	return mb
}

// Emit appends generated metrics to a pdata.MetricsSlice and updates the internal state to be ready for recording
// another set of data points. This function will be doing all transformations required to produce metric representation
// defined in metadata and user settings, e.g. delta/cumulative translation.
func (mb *MetricsBuilder) Emit(metrics pdata.MetricSlice) {
	mb.metricKafkaBrokers.emit(metrics)
	mb.metricKafkaConsumerGroupLag.emit(metrics)
	mb.metricKafkaConsumerGroupLagSum.emit(metrics)
	mb.metricKafkaConsumerGroupMembers.emit(metrics)
	mb.metricKafkaConsumerGroupOffset.emit(metrics)
	mb.metricKafkaConsumerGroupOffsetSum.emit(metrics)
	mb.metricKafkaPartitionCurrentOffset.emit(metrics)
	mb.metricKafkaPartitionOldestOffset.emit(metrics)
	mb.metricKafkaPartitionReplicas.emit(metrics)
	mb.metricKafkaPartitionReplicasInSync.emit(metrics)
	mb.metricKafkaTopicPartitions.emit(metrics)
}

// RecordKafkaBrokersDataPoint adds a data point to kafka.brokers metric.
func (mb *MetricsBuilder) RecordKafkaBrokersDataPoint(ts pdata.Timestamp, val int64) {
	mb.metricKafkaBrokers.recordDataPoint(mb.startTime, ts, val)
}

// RecordKafkaConsumerGroupLagDataPoint adds a data point to kafka.consumer_group.lag metric.
func (mb *MetricsBuilder) RecordKafkaConsumerGroupLagDataPoint(ts pdata.Timestamp, val int64, groupAttributeValue string, topicAttributeValue string, partitionAttributeValue string) {
	mb.metricKafkaConsumerGroupLag.recordDataPoint(mb.startTime, ts, val, groupAttributeValue, topicAttributeValue, partitionAttributeValue)
}

// RecordKafkaConsumerGroupLagSumDataPoint adds a data point to kafka.consumer_group.lag_sum metric.
func (mb *MetricsBuilder) RecordKafkaConsumerGroupLagSumDataPoint(ts pdata.Timestamp, val int64, groupAttributeValue string, topicAttributeValue string) {
	mb.metricKafkaConsumerGroupLagSum.recordDataPoint(mb.startTime, ts, val, groupAttributeValue, topicAttributeValue)
}

// RecordKafkaConsumerGroupMembersDataPoint adds a data point to kafka.consumer_group.members metric.
func (mb *MetricsBuilder) RecordKafkaConsumerGroupMembersDataPoint(ts pdata.Timestamp, val int64, groupAttributeValue string) {
	mb.metricKafkaConsumerGroupMembers.recordDataPoint(mb.startTime, ts, val, groupAttributeValue)
}

// RecordKafkaConsumerGroupOffsetDataPoint adds a data point to kafka.consumer_group.offset metric.
func (mb *MetricsBuilder) RecordKafkaConsumerGroupOffsetDataPoint(ts pdata.Timestamp, val int64, groupAttributeValue string, topicAttributeValue string, partitionAttributeValue string) {
	mb.metricKafkaConsumerGroupOffset.recordDataPoint(mb.startTime, ts, val, groupAttributeValue, topicAttributeValue, partitionAttributeValue)
}

// RecordKafkaConsumerGroupOffsetSumDataPoint adds a data point to kafka.consumer_group.offset_sum metric.
func (mb *MetricsBuilder) RecordKafkaConsumerGroupOffsetSumDataPoint(ts pdata.Timestamp, val int64, groupAttributeValue string, topicAttributeValue string) {
	mb.metricKafkaConsumerGroupOffsetSum.recordDataPoint(mb.startTime, ts, val, groupAttributeValue, topicAttributeValue)
}

// RecordKafkaPartitionCurrentOffsetDataPoint adds a data point to kafka.partition.current_offset metric.
func (mb *MetricsBuilder) RecordKafkaPartitionCurrentOffsetDataPoint(ts pdata.Timestamp, val int64, topicAttributeValue string, partitionAttributeValue string) {
	mb.metricKafkaPartitionCurrentOffset.recordDataPoint(mb.startTime, ts, val, topicAttributeValue, partitionAttributeValue)
}

// RecordKafkaPartitionOldestOffsetDataPoint adds a data point to kafka.partition.oldest_offset metric.
func (mb *MetricsBuilder) RecordKafkaPartitionOldestOffsetDataPoint(ts pdata.Timestamp, val int64, topicAttributeValue string, partitionAttributeValue string) {
	mb.metricKafkaPartitionOldestOffset.recordDataPoint(mb.startTime, ts, val, topicAttributeValue, partitionAttributeValue)
}

// RecordKafkaPartitionReplicasDataPoint adds a data point to kafka.partition.replicas metric.
func (mb *MetricsBuilder) RecordKafkaPartitionReplicasDataPoint(ts pdata.Timestamp, val int64, topicAttributeValue string, partitionAttributeValue string) {
	mb.metricKafkaPartitionReplicas.recordDataPoint(mb.startTime, ts, val, topicAttributeValue, partitionAttributeValue)
}

// RecordKafkaPartitionReplicasInSyncDataPoint adds a data point to kafka.partition.replicas_in_sync metric.
func (mb *MetricsBuilder) RecordKafkaPartitionReplicasInSyncDataPoint(ts pdata.Timestamp, val int64, topicAttributeValue string, partitionAttributeValue string) {
	mb.metricKafkaPartitionReplicasInSync.recordDataPoint(mb.startTime, ts, val, topicAttributeValue, partitionAttributeValue)
}

// RecordKafkaTopicPartitionsDataPoint adds a data point to kafka.topic.partitions metric.
func (mb *MetricsBuilder) RecordKafkaTopicPartitionsDataPoint(ts pdata.Timestamp, val int64, topicAttributeValue string) {
	mb.metricKafkaTopicPartitions.recordDataPoint(mb.startTime, ts, val, topicAttributeValue)
}

// Reset resets metrics builder to its initial state. It should be used when external metrics source is restarted,
// and metrics builder should update its startTime and reset it's internal state accordingly.
func (mb *MetricsBuilder) Reset(options ...metricBuilderOption) {
	mb.startTime = pdata.NewTimestampFromTime(time.Now())
	for _, op := range options {
		op(mb)
	}
}

// Attributes contains the possible metric attributes that can be used.
var Attributes = struct {
	// Group (The ID (string) of a consumer group)
	Group string
	// Partition (The number (integer) of the partition)
	Partition string
	// Topic (The ID (integer) of a topic)
	Topic string
}{
	"group",
	"partition",
	"topic",
}

// A is an alias for Attributes.
var A = Attributes
